experiment:
  number: 1
  seed: 42
  device: "cuda"
  date_format: '%A_%d_%B_%Y_%Hh_%Mm_%Ss'

dataset:
  name: "CIFAR100-N"       # Options: MNIST, CIFAR10, CIFAR100, CIFAR100-N, OxfordIIITPet
  batch_size: 64
  num_workers: 4
  data_dir: "../../../../data/fii-atnn-2024-project-noisy-cifar-100"
  mean: [0.485, 0.456, 0.406]
  std: [0.229, 0.224, 0.225]

model:
  name: "EfficientNetB0"       # MLP, ResNet18, ResNet50, ResNest14d, ResNest26d
  input_dim: 784         # Used only for MLP
  hidden_dim: 256
  num_classes: 100
  pretrained: True

training:
  epochs: 5
  milestones: [60, 120, 160]
  learning_rate: 0.001
  optimizer: "SGD"  # SGD, Adam, AdamW, Muon, SAM
  momentum: 0.9
  weight_decay: 5e-4
  scheduler: "CosineAnnealingWarmRestarts"  # stepLR, ReduceLROnPlateau, CosineAnnealingWarmRestarts
  step_size: 10
  gamma: 0.1
  mode: "min"
  verbose: True
  factor: 0.3
  patience: 3
  threshold: 0.09
  T_0: 781  # Number of iterations for the first restart (for CosineAnnealingWarmRestarts, ~1 epoch for CIFAR100 with batch_size=64)
  T_mult: 2  # A factor increases T_i after a restart (for CosineAnnealingWarmRestarts)
  eta_min: 1e-6  # Minimum learning rate (for CosineAnnealingWarmRestarts)
  loss_function: "CrossEntropyLoss"
  resume: False

logging:
  checkpoint_dir: "./checkpoints"
  log_interval: 100
  checkpoint: true
  save_epoch: 10

sweep:
  method: bayes
  metric:
    name: val_acc
    goal: maximize

  parameters:
    learning_rate:
      distribution: uniform
      min: 1e-4
      max: 3e-3

    batch_size:
      values: [32, 64, 128]

    weight_decay:
      distribution: uniform
      min: 0.0
      max: 0.005

    optimizer:
      values: ["SGD"]
